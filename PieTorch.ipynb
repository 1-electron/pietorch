{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/14/20; 1/16/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* everyone should know their children\n",
    "* include tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np \n",
    "from autograd import grad \n",
    "from autograd import elementwise_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "everything in PieTorch is a Tensor object.\n",
    "\n",
    "a Tensor object may contain an op. it contains an op when \n",
    "the Tensor is created from an operation, for example c = a + b. in \n",
    "this case, A and B are Tensors, and the addition operation creates\n",
    "C, a Tensor. C contains the addition op which allows it to compute\n",
    "partial derivatives dA/dC and dB/dC.\n",
    "\n",
    "a Tensor object knows its children. if A + B = C, then C is A's \n",
    "child. we keep track of children because backprogation starts at \n",
    "a leaf node, and the actual gradient for an update (dLoss/dX) must\n",
    "be accumulated by traversing through its children.\n",
    "\"\"\"\n",
    "\n",
    "class Tensor(object):\n",
    "    def __init__(self, val=0, parents=[], forward=None, \n",
    "                 name=None, op=None, terminal=True):\n",
    "        \n",
    "        self.val = float(val)\n",
    "        self.parents = parents  # list of Tensor objects\n",
    "        self.forward = forward  # forward function\n",
    "        self.grad = 0  # the value of its own gradient\n",
    "        self.name = name\n",
    "        self.op = op\n",
    "        self.terminal = terminal\n",
    "        self.children = []  # list of Tensor objects\n",
    "        \n",
    "        # when Tensor is instantiated as result of an operation, it will have parents\n",
    "        if len(self.parents) > 0:\n",
    "            self._update_parent()\n",
    "        \n",
    "    def _update_parent(self):\n",
    "        \"\"\"\n",
    "        if a Tensor is instantiated with parents, then it will also\n",
    "        update its parents, so its parents know its child.\n",
    "        \"\"\"\n",
    "        for parent in self.parents:\n",
    "            parent.children.append(self)\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        update its parents by their corresponding gradients.\n",
    "        \"\"\"\n",
    "        if self.terminal is True:\n",
    "            pass\n",
    "        else:\n",
    "            # compute gradients for its parents\n",
    "            gradients = self.op.backward()  # returns a list of gradients\n",
    "            \n",
    "            # now update each parent with its corresponding gradient\n",
    "            for i in range(len(self.parents)):\n",
    "                self.parents[i].grad = gradients[i]\n",
    "                \n",
    "            # recursively call backward() on its parents\n",
    "            for T in self.parents:\n",
    "                T.backward()\n",
    "                \n",
    "    def _update(self):\n",
    "        \"\"\"called by optimizer.\"\"\"\n",
    "        self.val = self.val - self.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add is an operation. when Add is called, an Adder object is \n",
    "created and used to instantiate a Tensor object, which is \n",
    "then returned to caller.\n",
    "\"\"\"\n",
    "class ADD(object):\n",
    "    def __new__(self, x, y):  # https://stackoverflow.com/questions/53485171/how-to-return-objects-straight-after-instantiating-classes-in-python\n",
    "        op = _ADDER(x, y)\n",
    "        return Tensor(val=op.forward(), parents=[x, y], op=op, terminal=False)\n",
    "        \n",
    "\"\"\"\n",
    "the ADDER object will contain methods/attributes for the ADD\n",
    "operation. certain ADDER attributes and methods will be used\n",
    "to instantiate a TENSOR object. it is this TENSOR object that\n",
    "is subsequently returned.\n",
    "\n",
    "we used the adder object to organize node state such as \n",
    "gradient. it is not necessary but it makes the code more \n",
    "readable.\n",
    "\"\"\"\n",
    "class _ADDER(object):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = float(x.val)\n",
    "        self.y = float(y.val)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.x + self.y\n",
    "\n",
    "    def f(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "    def backward(self):\n",
    "        # return a list of gradients\n",
    "        self.df_dx = elementwise_grad(self.f, 0)\n",
    "        self.df_dy = elementwise_grad(self.f, 1)\n",
    "        return [self.df_dx(self.x, self.y), self.df_dy(self.x, self.y)]\n",
    "\n",
    "class MULTIPLY(object):\n",
    "    def __new__(self, x, y):\n",
    "        \"\"\"\n",
    "        x and y are Tensor objects.\n",
    "        \"\"\"\n",
    "        op = _MULTIPLIER(x, y)  # bundle everything into a multiplier object, to be passed\n",
    "        return Tensor(val=op.forward(), parents=[x, y], op=op, terminal=False)\n",
    "        \n",
    "class _MULTIPLIER(object):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = float(x.val)\n",
    "        self.y = float(y.val)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.x * self.y\n",
    "\n",
    "    def f(self, a, b):\n",
    "        return a * b\n",
    "\n",
    "    def backward(self):\n",
    "        # partial derivatives https://github.com/HIPS/autograd/issues/437\n",
    "        self.df_dx = elementwise_grad(self.f, 0)\n",
    "        self.df_dy = elementwise_grad(self.f, 1)\n",
    "        return [self.df_dx(self.x, self.y), self.df_dy(self.x, self.y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating parents:  3.0\n",
      "updating parents:  -12.0\n"
     ]
    }
   ],
   "source": [
    "X = Tensor(val=-2, name=\"X\")\n",
    "Y = Tensor(val=5, name=\"Y\")\n",
    "Q = ADD(X, Y)\n",
    "Z = Tensor(val=-4, name=\"Z\")\n",
    "F = MULTIPLY(Q, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.children[0].val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Tensor at 0x10f2ad550>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1.), array(1.), -4.0, 3.0, 0)"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.grad, Y.grad, Q.grad, Z.grad, F.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTIMIZER():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # find all Tensors which will be updated\n",
    "        # https://stackoverflow.com/questions/633127/viewing-all-defined-variables\n",
    "        self.ls_tensors = [v for k, v in globals().copy().items() if type(v) is TENSOR]\n",
    "        \n",
    "    def step(self):\n",
    "        for T in self.ls_tensors:\n",
    "            \n",
    "            # if T is a terminal node, that means it is a learnable weight \n",
    "            if T.terminal:\n",
    "                T._update()\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = OPTIMIZER()\n",
    "O.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.val, Y.val, Q.val, Z.grad, F.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
